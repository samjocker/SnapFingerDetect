視覺辨識彈指偵測
===
### 製作時間：高一
## 由來
本來是想要做智慧眼鏡而開發的手勢操控，本來預想有彈指開燈等等應用場景，可惜因為當時經費不足而停止開發，甚至當時手勢還想要多做點兩下、捏等手勢動作，如果當時完成了可能會有現在 [Apple Vision Pro](https://www.apple.com/tw/newsroom/2023/06/introducing-apple-vision-pro/ "蘋果官網") 5%
的相似度吧😅
## 詳細原理
1. 使用opencv讀取鏡頭一幀畫面
2. 利用mediapipe手部關鍵點模型來得出關鍵點座標
3. 讀取食指中指大拇指的頂端座標看是否很接近以及無名指是否與食指離很遠 *（圖一）*
4. 如果都是那接著再判斷中指是否距離食指很遠且兩個動作經過時間在0.3秒內判定為一個完整的彈指程式 *（圖二）*
## 智慧眼鏡解決方案
因為有考慮到直接放一個螢幕在眼鏡前面會因靠太近而導致要看畫面時眼睛要特別用力對焦無法輕鬆觀看，並且本來是想要用LCD螢幕將背光板拆掉來達成透命螢幕的效果，但能不能成功以及透明度表現都是未知數，所以因為這幾個貧頸最後想說把螢幕放置在眼鏡鏡腿尾端然後前端用玻璃反射螢幕光源出來至鏡片上，但即便顯示有了解決方案但還有電池、處理器等問題要解決，當時的我們打算牽訊號線至腰帶上掛著電池及樹莓派的解決方案，可惜當時樹莓派對我們來說有點爆預算😅，不過電池部分也與現在的 [Apple Vision Pro](https://www.apple.com/tw/newsroom/2023/06/introducing-apple-vision-pro/ "蘋果官網") 有著相同的解決方案，看到發佈會有種沾沾自喜的感覺😂
## 心得
此次手勢辨識算是我初學python的練習專案吧，現在來看雖然很多吐槽點，但是我還忘不了當初彈完手指時終端機print `snap` 時的雀躍感，這個項目也讓我首次接觸到了Google的這個簡單易使用且精準的機器學習模型，在後續我也常常使用mediapipe來快速製作一些有趣的東西，這個項目雖然沒有完整把智慧眼鏡做出來，但我也學到了很多東西例如智慧眼鏡技術貧頸在哪裡、知道要讓這麼小的眼鏡有高算力是很難的技術等等的，如果把這個項目給現在的我做的話可能會放esp32 cam在眼鏡上並把畫面回傳給我用Swift寫的ios app上處理吧，這樣既可以使用高效能的A系列處理器又能操控其他手機app之類的，比之前樹莓派解法優勢多太多。

---

Apple Vision Pro : 一個由蘋果公司生產的混合現實頭戴式電子裝置

mediapipe : 由Goole推出的簡單易用高效能機器學習工具

esp32 cam : 低成本低功耗由台積電40奈米代工之含鏡頭開發板